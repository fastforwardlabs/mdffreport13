<!DOCTYPE html>
    <html lang="en">
      <head>
    <meta charset="utf-8" />

    <title>Causality for Machine Learning</title>
    <meta name="description" content="TODO" />

    <meta property="og:title" content="Causality for Machine Learning" /> 
    <meta property="og:description" content="TODO" />
    <meta property="og:image" content="https://ff13.fastforwardlabs.com/ff13-share.jpg" />
    <meta property="og:url" content="https://ff12.fastforwardlabs.com" />
    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="viewport" content="width=device-width" />
    <link rel="icon" type="image/x-icon" href="https://ff13.fastforwardlabs.com/favicon.ico" />
    
    <style type="text/css">
    
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Bold.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Bold.woff') format('woff');
    font-weight: bold;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-BoldItalic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-BoldItalic.woff') format('woff');
    font-weight: bold;
    font-style: italic;
  }
  
    * {
      box-sizing: border-box;
    }
    html {
      background: #fff;
      font-family: "Plex Sans", serif, sans-serif;
      font-size: 17.5px;
      line-height: 28px;
    }
    body {
      margin: 0;
    }
    .content {
      max-width: 64ch;
      padding-left: 2ch;
      padding-right: 2ch;
      margin: 0 auto;
      display: block;
      padding-bottom: 0px;
    }
   p, ul, ol {
      margin: 0;
    }
    ul, ol {
      padding-left: 3ch;
    }
  p {
   // text-indent: 3ch;
}
    li p:first-child {
      text-indent: 0;
    }
 
   hr {
      margin: 0;
      border-top-color: black;
      margin-top: -0.5px;
      margin-bottom: 27.5px;
    }
  
h1, h2, h3, h4, h5, h6, button { font-size: inherit; line-height: inherit; font-style: inherit; font-weight: inherit; margin: 0; font-feature-settings: "tnum"; border: none; background: transparent; padding: 0;  }
button:focus, button:hover {
  background: rgba(0,0,0,0.125);
  outline: none;
}
h1 {
  font-size: 42px;
  line-height: 56px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h2 {
  font-size: 31.5px;
  line-height: 42px;
  font-weight: bold;
  margin-top: 28px;
  margin-bottom: 14px;
}
h3 {
  font-size: 26.25px;
  line-height: 35px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h4 {
  font-size: 21px;
  line-height: 28px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h5 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-weight: bold;
}
h6 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-style: italic;
}
p {
  margin-bottom: 14px;
}
.content {
  position: relative;
  }
figure {
  margin: 0;
  margin-top: 14px;
  margin-bottom: 28px;
  display: block;
  position: relative;
  page-break-inside: avoid;
}
blockquote {
  margin: 0;
   margin-top: 14px;
  margin-bottom: 14px;
margin-left: 2ch;
}
blockquote + blockquote {
  margin-top: 0;
}
figcaption {
  font-family: "Plex Mono", serif, monospace;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 21px;
}
.info {
  background: #efefef;
  padding-left: 2ch;
  padding-right: 2ch;
  padding-top: 14px;
  padding-bottom: 14px;
  margin-bottom: 28px;
}
.info p:last-child {
  margin-bottom: 0;
}
img {
  display: block;
  position: relative;
  max-width: 100%;
  margin: 0 auto;
  page-break-inside: avoid;
}

table {
  min-width: 100%;
  text-align: left;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 18.900000000000002px;
  border-collapse: collapse;
}
table, th, td {
  border: solid 1px black;
}
td {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  valign: top;
  vertical-align: top;
}
th {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  vertical-align: top;
  background: #efefef;
}
table ul, table ol {
  list-style-position: inside;
  padding-left: 0;
}

  a {
    color: inherit;
  }
  .table-of-contents {
    background: #efefef;
    position: fixed;
    left: 0;
    top: 0;
    width: 32ch;
    height: 100vh;
    overflow-y: auto;
    background: #efefef;
      // background: rgba(230,230,230,0.85);
      //   backdrop-filter: blur(5px);
  }
  body {
    padding-left: 32ch;
  }
  p:empty {
    display: none;
  }

  #report-iso {
    display: none;
  }

.table-of-contents {
    counter-reset: chapters;
}
 .table-of-contents ul {
    list-style: none;
    padding-left: 0;
  }
 .table-of-contents > ul {
    padding-bottom: 28px;
  }
  .table-of-contents > ul > li > a:before {
          counter-increment: chapters;
          content: counter(chapters) ". ";
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 
 .table-of-contents > ul > li > ul > li {
    font-weight: normal;
    font-style: normal;
    text-transform: none;
    letter-spacing: 0;
    margin-left: 0;
  }
 .table-of-contents > ul > li > ul > li > ul > li {
    font-weight: normal;
    font-style: italic;
  }
 .table-of-contents a {
    text-decoration: none;
  }
  .table-of-contents a:hover {
    text-decoration: underline;
  }
 sup {
  }
  .table-of-contents ul a {
    display: block;
    padding-left: 3ch;
    text-indent: -1ch;
    padding-right: 2ch;
  }
  .table-of-contents ul li a.active {
    position: relative;
    background: #ddd;
    // text-decoration: line-through;
  }

 .table-of-contents > ul > li > ul > li > a {
    font-size: 15.75px;
      line-height: 25.2px;
    // padding-left: 4ch;
  }
  .table-of-contents > ul > li > ul > li > ul > li > a {
    padding-left: 5ch;
  }

h1 {
    counter-reset: chp;
}
h2 {
  position: relative;
  display: block;
  page-break-before: always;
  padding-top: 42px;
}
  h2:before {
    position: absolute;
    left: 0;
    top: 0;
      font-size: 17.5px;
    color: black;
    counter-increment: chp;
    content: "chapter " counter(chp);
    text-transform: uppercase;
  }
 
  .toc-desktop-hidden .table-of-contents {
    width: auto;
  }
  .toc-desktop-hidden #contents-label {
    display: none;
  }
  .toc-desktop-hidden .table-of-contents ul {
    display: none;
  }
  body.toc-desktop-hidden {
    padding-left: 5ch;
  }
  body:before {
    content: " ";
    height: 28px;
    width: 96ch;
    background: black;
    position: absolute;
    left: 0;
    top: 0;
    z-index: 999;
    display: none;
  }
    #toc-header {
      margin-top: 14px;
      margin-bottom: 14px;
      margin-left: 1ch;
      margin-right: 1ch;
    }
 
  @media screen and (max-width: 1028px) {
    h1 {
      font-size: 36.75px;
      line-height: 49px;
      font-weight: bold;
      margin-top: 14px;
      margin-bottom: 14px;
    }
    .table-of-contents ul li {
      padding-top: 3.5px;
      padding-bottom: 3.5px;
    }

    #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
    }
 
    body {
      padding-left: 0;
      padding-top: 42px;
    }
    #contents-label {
      display: none;
    }
    .table-of-contents {
      height: auto;
      width: 100%;
      z-index: 3;
    }
  body.toc-mobile-show .content:before {
      content: "";
      position: fixed;
      left: 0;
      top: 0;
      bottom: 0;
      right: 0;
      background: rgba(0,0,0,0.25);
      z-index: 2;
      border-top: solid 42px #aaa;
    }
 
    .table-of-contents > ul {
      display: none;
    }
   body.toc-mobile-show {
      overflow: hidden;
    }
    body.toc-mobile-show #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
      position: relative;
    }
    body.toc-mobile-show .table-of-contents {
      width: 32ch;
      height: 100vh;
      max-width: calc(100% - 4ch);
      overflow: auto;
    }
   body.toc-mobile-show .table-of-contents > ul {
      display: block;
      padding-bottom: 28px;
      position: relative;
    }
    body.toc-mobile-show #contents-label {
      display: inline;
      position: relative;
    }
  }
}
</style>
    <script>
    function inViewport(elem) {
      let bounding = elem.getBoundingClientRect();
      return (
        bounding.top >= 0 &&
        bounding.left >= 0 &&
        bounding.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        bounding.right <= (window.innerWidth || document.documentElement.clientWidth)
      );
    };

    function setActive(target_id) {
      let selector = '.table-of-contents ul li a[href="#' + target_id + '"]'
      let link = document.querySelector(selector)
      if (link !== null) {
        link.className = 'active'
      }
    }

    window.addEventListener("load", (event) => {
      let headings = document.querySelectorAll('h2, h3');
      let links = document.querySelectorAll('.table-of-contents ul li a')

      observer = new IntersectionObserver((entry, observer) => {
        if (entry[0].intersectionRatio === 1) {
          for (let link of links) {
            link.className = ''
          }
          let target_id = entry[0].target.getAttribute('id')
          setActive(target_id)
        }
      }, { threshold: 1, rootMargin: "0px 0px -50% 0px" });

      let first = true
      for (let heading of headings) {
        if (first && inViewport(heading)) {
          setActive(heading.getAttribute('id'))
          first = false
        }
        observer.observe(heading);
      }

      document.querySelector('#toggle_contents').addEventListener('click', () => {
        let body = document.body
        if (window.innerWidth > 1027) {
          let hidden_class = "toc-desktop-hidden"
          if (body.className === hidden_class) {
            body.className = ''
          } else {
            body.className = hidden_class
          }
        } else {
          let show_class = "toc-mobile-show"
          if (body.className === show_class) {
            body.className = ''
          } else {
            body.className = show_class
          }
        }
      })

      for (let link of links) {
        link.addEventListener('click', (e) => {
          let href = e.target.getAttribute('href')
          let elem = document.querySelector(href)
          window.scroll({
            top: elem.offsetTop - 28,
            left: 0,
            behavior: 'smooth'
          })
          if (window.innerWidth < 1028) {
            document.body.className = ''
          }
          e.preventDefault() 
        })
      }

      document.querySelector('.content').addEventListener('click', () => {
        if (window.innerWidth < 1028) {
          document.body.className = ''
        }
      })
      document.querySelector('.table-of-contents').addEventListener('click', (e) => {
        e.stopPropagation()
      })

      let mediaQueryList = window.matchMedia("(max-width: 1028px)");
      function handleBreakpoint(mql) {
        // clear any left over toggle classes
        document.body.className = ''
      }
      mediaQueryList.addListener(handleBreakpoint);
    }, false);
  </script>

 </head>
      <body>
        <div class="content" style="position: relative;">
          <div style="margin-top: 28px; line-height: 0; display: flex;">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward" style="display: block; height: 14px; margin-bottom: 7px;" src='/figures/cloudera-fast-forward-logo.png' /></a>
          </div>
          <h1 id="causality-for-machine-learning">Causality for Machine Learning</h1>
<p><div class="table-of-contents"><div id="toc-header" style="display: flex; font-weight: bold; text-transform: uppercase;">
     <div><button id="toggle_contents" style="padding-left: 0.5ch; padding-right: 0.5ch; cursor: pointer; position: relative; top: -1px;">☰</button><span id="contents-label" style="margin-left: 0;"> Contents</span></div>
  </div><ul><li><a href="#intro">Intro</a></li><li><a href="#background%3A-causal-inference">Background: Causal Inference</a><ul><li><a href="#why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</a></li><li><a href="#the-ladder-of-causation">The ladder of causation</a></li><li><a href="#from-correlation-to-causation">From correlation to causation</a></li><li><a href="#from-prediction-to-intervention">From prediction to intervention</a></li><li><a href="#how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</a></li><li><a href="#recap">Recap</a></li></ul></li><li><a href="#causality-and-invariance">Causality and invariance</a><ul><li><a href="#the-great-lie-of-machine-learning">The great lie of machine learning</a></li><li><a href="#dangers-of-spurious-correlations">Dangers of spurious correlations</a></li><li><a href="#invariance">Invariance</a></li><li><a href="#invariant-causal-prediction">Invariant Causal Prediction</a></li><li><a href="#invariant-risk-minimization">Invariant Risk Minimization</a></li><li><a href="#how-irm-works">How IRM works</a></li></ul></li><li><a href="#prototype">Prototype</a></li><li><a href="#landscape">Landscape</a><ul><li><a href="#use-cases">Use Cases</a></li><li><a href="#tools">Tools</a></li></ul></li><li><a href="#ethics">Ethics</a><ul><li><a href="#causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</a></li><li><a href="#omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</a></li><li><a href="#invariance-as-a-route-to-fairness">Invariance as a route to fairness</a></li></ul></li><li><a href="#future">Future</a><ul><li><a href="#comparable-approaches">Comparable approaches</a></li><li><a href="#looking-ahead">Looking ahead</a></li></ul></li></ul></div></p>
<h2 id="intro">Intro</h2>
<p>Blah.</p>
<h2 id="background%3A-causal-inference">Background: Causal Inference</h2>
<p>In this chapter, we aim to familiarize the reader with the essentials of causal reasoning, especially in how it differs from supervised learning. In particular, we give an informal introduction to structural causal models. Grasping the basic notions of causal modeling allows for a much richer understanding of invariance and generalization, which we discuss in the next Chapter.</p>
<h3 id="why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</h3>
<p>Imagine a bank that would like to reduce the number of business loans which default. Historical data and sophisticated supervised learning techniques may be able to accurately identify which loans are likely to default, and interpretability techniques may tell us some features that are correlated with (or predictive of) defaulting. However, to reduce the default rate, we must understand what changes to make, which requires understanding not only <em>which</em> loans default, but <em>why</em> the loans default.</p>
<p>[figure: bank]</p>
<p>It may be that we find small loans are more likely to default than larger loans. One might naively assume that the bank ought to stop making small loans. However, perhaps it is really the case that smaller businesses are more likely to fail than large businesses, and <em>also</em> more likely to apply for small loans. In this case, the true causal relationship is between the size of the <em>business</em> and defaulting, not between the size of the <em>loan</em> and defaulting. If so, we could have made a poor policy decision about loan size, rather than business size.</p>
<p>Unfortunately, supervised learning alone cannot tell us which is true. If we include both loan size and business size as features in our model, we will simply find that they are both related to loan defaulting, to some extent. While that insight is true - as they are both statistically related to defaulting - which is <em>causing</em> defaulting is a separate question, and the one to which we want the answer.</p>
<p>Causality gives us a framework to reason about such questions, and recent developments at the intersection of causality and machine learning are making the discovery of such causal relationships easier.</p>
<h4 id="the-shortcomings-of-supervised-learning">The shortcomings of supervised learning</h4>
<p>Supervised machine learning has proved enormously successful at some tasks - particularly in dealing with tasks that require high-dimensional inputs, such as computer vision and natural language processing. There has been truly remarkable progress over the past two decades, and it should be noted that acknowledgment of the shortcomings of supervised learning does not diminish that progress.</p>
<p>With its success has come inflated expectations of autonomous systems, inferring they could be capable of independent decision-making and even human-like intelligence. Current machine learning approaches are unable to meet those expectations, owing to fundamental limitations of pattern recognition.</p>
<p>One such limitation is <strong>generalizability</strong> (also called <em>robustness</em> or <em>adaptability</em>), that is, the ability to apply a model learned in one context in a new environment. Many current state-of-the-art machine learning approaches assume that the trained model will be applied to data that looks the same as the training data. These models are trained on highly specific tasks, like recognizing dogs in images or identifying fraud in banking transactions. In real life, though, the data on which we predict is often different from the data on which we train, even when the task is the same. For example, training data is often subject to some form of selection bias, and simply collecting more of it does not mitigate that.</p>
<p>[figure: generalizability]</p>
<p>Another limitation is <strong>explainability</strong>, that is, machine learning models remain mostly “black boxes” unable to explain the reasons behind their predictions or recommendations, thus eroding users’ trust and impeding diagnosis and repair. For example, a deep learning system can be trained to recognize cancer in medical images with high accuracy, provided it is given plenty of images and compute power, but - unlike a real doctor - it cannot explain why or how a particular image suggests disease. Several methods for understanding model predictions have been developed, and while these are necessary and welcome, understanding the interpretation and limitations of their outputs is a science in itself. While model interpretation methods like <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1705.07874">SHAP</a> are useful, they provide insight only into how the model works, not how the world works.</p>
<p>[figure: explainability]</p>
<p>And finally, the understanding of <strong>cause-and-effect</strong> connections - a key element of human intelligence - is absent from pattern recognition systems. Humans have the ability to answer “what if” kinds of questions. What if I change something? What if I had acted differently? Such interventional, counterfactual or retrospective questions are the forté of human intelligence. While imbuing machines with this kind of intelligence is still far-fetched, researchers in deep learning are increasingly recognizing the importance of these questions, and using them to inform their research.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>[figure: cause and effect]</p>
<p>All of this means that supervised machine learning systems must be used cautiously in certain situations. And if we want to mitigate these restrictions effectively, causation is key.</p>
<h4 id="what-does-causality-bring-to-the-table%3F">What does causality bring to the table?</h4>
<p>Causal inference provides us with tools that allow us to answer the question of <em>why</em> something happens. This takes us a step further than traditional statistical or machine learning approaches that are focused on predicting outcomes and concerned with identifying associations.</p>
<p>Causality has long been of interest to humanity on a philosophical level, but it has only been in the latter half of the 20th century (thanks to the work of pioneering methodologists such as Donald Rubin and Judea Pearl), that a mathematical framework for causality has been introduced. In recent years, the boom of machine learning has enhanced the development of causal inference and attracted new researchers to the area.</p>
<p>Identifying causal effects helps us understand a variety of things: for example, user behavior in online systems<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, effect of social policies, risk factors of diseases. Questions of cause-and-effect are also critical for the design of data-driven applications. For instance, how do algorithmic recommendations affect our purchasing decisions? How do they affect a student’s learning outcome or a doctor’s efficacy? All of these are hard questions and require thinking about the counterfactual: what would have happened in a world with a different system, policy, or intervention?  Without causal reasoning, correlation-based methods can lead us astray.</p>
<p>That said, learning causality is a challenging problem. There are broadly two situations in which we could find ourselves: in one case, we are able to actively intervene in the system we are modeling and get experimental data; in the other, we have only observational data.</p>
<p>The gold standard in establishing causal effects is a Randomised Controlled Trial (RCT) and this falls under the experimental data category. In an RCT, we try to engineer similar populations using random assignment (as choosing the populations manually could introduce selection effects that destroy our ability to learn causal relations) and apply an intervention to one population and not the other. From this, we measure the causal effect of changing one variable as a simple difference in the quantity of interest between the two populations.</p>
<p>We can use RCTs to establish whether a particular causal relation holds. However, trials are not always physically possible, and even when they are, they are not always ethical (for instance, it would not be ethical to deny a patient a treatment that is reasonably believed to work, or trial a news aggregation algorithm designed to influence a person’s mood without informed consent).<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> In some cases, we can find naturally occurring experiments. In the worst case, we’re left trying to infer causality from observational data alone.</p>
<p>In general, this is not possible, and we must at least impose some modeling assumptions. There are several formal frameworks for doing so. For our purpose of building intuition, we’ll introduce the <a href="http://bayes.cs.ucla.edu/BOOK-2K/">Structural Causal Model</a> (SCM) framework of Pearl in this chapter.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h3 id="the-ladder-of-causation">The ladder of causation</h3>
<p>In <a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why</a>, Judea Pearl, author of much foundational work in causality, describes three kinds of reasoning we can perform as the rungs on a ladder. These rungs describe when we need causality, and what it buys us.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></p>
<p>[figure: ladder]</p>
<p>On the <strong>first rung</strong>, we can do <strong>statistical and predictive reasoning</strong>. This covers most (but not all) of what we do in machine learning. We may make very sophisticated forecasts, infer latent variables in complex deep generative models, or cluster data according to subtle relations - all of these things sit on rung one.</p>
<p><em>Example: a bank wishes to predict which of its current business loans are likely to default, so it can make financial forecasts accounting for likely losses.</em></p>
<p>The <strong>second rung</strong> is <strong>interventional reasoning</strong>. Interventional reasoning allows us to predict what will happen when a system is changed. This enables us to describe what characteristics are particular to the exact observations we’ve made, and what should be invariant across new circumstances. This kind of reasoning requires a <em>causal</em> model. Intervening is a fundamental operation in causality, and we’ll discuss both interventions and causal models in this chapter.</p>
<p><em>Example: a bank would like to reduce the number of loans which default, and considers changing its policy. Predicting what will happen as a result of this intervention requires that the bank understand the causal relations which affect loan defaulting.</em></p>
<p>The <strong>third rung</strong> is <strong>counterfactual reasoning</strong>. On this rung, we can talk not only about what has happened, but also what would have happened if circumstances were different. Counterfactual reasoning requires a more precisely specified causal model than intervention. This form of reasoning is very powerful, providing a mathematical formulation of computing in alternate worlds where events were different.</p>
<p><em>Example: a bank would like to know what the likely return on a loan would have been, had they offered different terms to what they did.</em></p>
<p>By now, we hopefully agree that there is something to causality, and it has much to offer. We have yet to really define causality. We must begin with that most familiar refrain: correlation is not causation.</p>
<h3 id="from-correlation-to-causation">From correlation to causation</h3>
<h4 id="spurious-correlations">Spurious correlations</h4>
<p>Very many things display correlation. The rooster crows when the sun rises.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> The lights turn off when you flick a switch. Global temperatures have risen alarmingly since the 1800s, and meanwhile pirate numbers have dwindled to almost nothing (<a href="https://www.forbes.com/sites/erikaandersen/2012/03/23/true-fact-the-lack-of-pirates-is-causing-global-warming/#5cb710453a67">Forbes</a>). These examples show us that while correlation can <em>appear</em> as a result of causation, as in the case of the light switch, correlation certainly does not always <em>imply</em> causation, as in the case of the pirates.</p>
<p>Correlated things are not always related.<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> It’s possible to find many correlations with no readily imaginable causal interaction. The internet treasure <a href="https://www.tylervigen.com/spurious-correlations">Spurious Correlations</a> collects many amusing examples of this. These spurious correlations most likely arise as a result of small sample size and coincidences that are bound to happen when making many comparisons. We should not be surprised if we find something that has low probability if we try many combinations.</p>
<p>[figure: spurious correlation]</p>
<p>In real world systems, spurious correlations can be cause for serious ethical concerns. For instance, certain characteristics may be associated with individuals or minority groups which make superficial features powerful at a learning task. This can easily embed bias and unfairness into an algorithm based on spurious correlations in a given dataset.</p>
<h4 id="the-principle-of-common-cause">The Principle of Common Cause</h4>
<p>In a posthumous 1956 book, <a href="https://www.goodreads.com/book/show/848892.The_Direction_of_Time">The Direction of Time</a>, Hans Reichenbach outlined the principle of common cause. He states the principle this way:</p>
<blockquote>
<p>“If an improbable coincidence has occurred, there must exist a common cause.”</p>
</blockquote>
<p>Our understanding of causality has evolved, but this language is remarkably similar to what we use now. Let’s discuss how correlation may arise from causation.</p>
<p>We will do this in the framework of Structural Causal Models (SCMs). An SCM is a directed acyclic graph of relationships between variables. The nodes represent variables, and the edges between them point from cause to effect. The value of each variable depends only on its direct parents in the graph (the other variables which point directly into it) and a noise variable encapsulating any environmental interactions we are not modeling. We will examine three fundamental causal structures.</p>
<div class="info">
<h5 id="causal-terminology">Causal Terminology</h5>
<p>A causal graph is a directed acyclic graph denoting the dependency between variables.</p>
<p>A structural causal model carries more information than a causal graph alone - it also specifies what the functional form of dependencies between variables.</p>
<p>Remarkably, it’s possible to do much causal reasoning, including calculating the size of causal effects, via the graph alone, without specifying a parametric form for the relationships between causes and effects.</p>
</div>
<h5 id="1.-direct-causation">1. Direct causation</h5>
<p>The simplest way in which correlation between two variables arises is when one variable is a direct cause of the other. We say that one thing causes another when a change in the first thing, while holding everything else constant, results in a change in the second. In the business loan defaulting example discussed earlier, this could mean we could create a two node graph with one of the nodes being whether or not a business is small (say “small business” with values 0 or 1) and the other node being “default” indicating whether or not the business defaulted on the loan. In this case, we would expect that a small business increases the chances of it defaulting.</p>
<p>[figure: direct causation]</p>
<p>This setup is immediately reminiscent of supervised learning, where we have a dataset of features, X, and targets, Y, and want to learn a mapping between them. However, in machine learning, we typically start with all available features and select those that are most informative about the target. When drawing a causal relationship, only those features we believe have an actual causal effect on the target should be included as direct causes. As we will see below, there are other diagrams that can lead to a predictive statistical relationship between X and Y in which neither directly causes the other.</p>
<h5 id="2.-common-cause">2. Common cause</h5>
<p>A common pattern is for a single variable to be the cause of multiple other variables. If a variable Z is a direct cause of both X and Y, we say that Z is a common cause and call the structure a “fork.” For example, unemployment could potentially cause both loan default and reduced consumer spend.</p>
<p>[figure: fork]</p>
<p>Because both consumer spend and loan default depend on unemployment , they will appear correlated. A given value of unemployment will generate some values of consumer spend and loan default, and when unemployment changes, both consumer spend and loan default will change. As such, in the joint distribution of the SCM, the two dependent variables, consumer spend and loan default, will appear statistically related to one another.</p>
<p>However, if we were to <em>condition</em> on unemployment (for instance, by selecting data corresponding to a fixed unemployment rate), we would see that consumer spend and loan default are independent from one another.</p>
<p>The common cause unemployment <em>confounds</em> the relationship between consumer spend and loan default. We are unable to correctly calculate the relationship between consumer spend and loan default without accounting for unemployment (by conditioning). This is especially dangerous if unnoticed.</p>
<p>Unfortunately, confounders can be tricky or impossible to detect from observational data alone. In fact, if we look only at consumer spend and loan default, we could see the same joint distribution as in the case where consumer spend and loan default are directly causally related. As such, we should think of causal graphs as encoding our <em>assumptions</em> about the system we are studying. We return to this point in [TODO: link] How do we know which graph to use?</p>
<h5 id="3.-common-effect">3. Common effect</h5>
<p>The opposite common pattern is for one effect to have multiple direct causes. A node that has multiple causal parents is called a “collider” with respect to those nodes.</p>
<p>A collider is a node that depends on more than one cause. In this example, loan defaulting depends on both commercial credit score and number of liens (a “lien” refers to the right to keep possession of property belonging to another entity until a debt owed by that entity is discharged), so we call loan default a <em>collider</em>.</p>
<p>Colliders are different to chains of direct causation and forks because the conditioning behaviour works oppositely. Before any conditioning, commercial credit score and number of liens are unconditionally independent. There is no variable with causal arrows going into both commercial credit score and number of liens, and no arrow linking them directly, so we should not expect a statistical dependency. However, if we condition on the collider, we will induce a conditional dependence between commercial credit score and number of liens.</p>
<p>This may seem a little unintuitive, but we can make sense of it with a little thought experiment. Loan default depends on both commercial credit score and number of liens, so if either of those changes value, the chance of loan default changes. We fix the value of loan default (say, we look only at those loans that did default). Now, if we were to learn anything about the value of commercial credit score, we would know something about the number of liens too; only certain values of number of liens are compatible with the conditioned value of loan defaulting and observed value of commercial credit score. As such, conditioning on a collider induces a spurious correlation between the parent nodes. Conditioning on a collider is exactly selection bias!</p>
<h4 id="structural-causal-models%2C-in-code">Structural Causal Models, in code</h4>
<div class="info">
<p>The small causal graphs shown above are an intuitive way to reason about causality. Remarkably, we can do much causal reasoning (and calculate causal effects) simply by specifying qualitatively which variables causally influence others with these graphs. In the real world, causal graphs can be large and complex.</p>
<p>Of course, there are other ways to encode the information. Given the graph, we can easily write down an expression for the joint distribution: it’s the product of probability distributions for each node conditioned on its direct causal parents. In the case of a collider structure, <code>x</code> → <code>z</code> ← <code>y</code>, the joint distribution is simply <code>p(x,y,z) = p(x) p(y) p(z|x,y)</code>. The conditional probability <code>p(z|x,y)</code> is exactly what we’re used to estimating in supervised learning!</p>
<p>If we know more about the system, we can move from this causal graph to a full structural causal model. An example SCM compatible with this graph would be:</p>
<hr>
<pre><code class="language-python">from numpy.random import randn

def x():
  return -5 + randn()

def y():
  return 5 + randn()

def z(x, y):
  return x + y + randn()

def sample():
  x_ = x()
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<hr>
<p>Each of the variables has an independent random noise associated with it, arising from factors not modeled by the graph. These distributions need not be identical, but must be independent. Notice that the structure of the graph encodes the dependencies between variables, which we see as the function signatures. The values of <code>x</code> and <code>y</code> are independent, but <code>z</code> depends on both. We can also see clearly that the model defines a generative process for the data, since we can easily sample from the joint distribution by calling the <code>sample</code> function. Doing so repeatedly allows us to chart the joint distribution, and see that <code>x</code> and <code>y</code> are indeed independent; there’s no apparent correlation in the scatter chart.</p>
<p>[figure: histogram and scatter plot]
Fig. Histogram of the distribution of x, y and z, and scatter plot of the joint distribution of x and y.</p>
<p>Now that we have a model in code, we can see a selection bias effect. If we condition the data to only values of <code>z</code> (the collider node) greater than a cutoff (which we can do easily, if inefficiently, by filtering the samples to those where <code>z &gt; 2.5</code>), the previously independent <code>x</code> and <code>y</code> become negatively correlated.</p>
</div>
<h3 id="from-prediction-to-intervention">From prediction to intervention</h3>
<p>Now that we have some understanding of what a causal model is, we can get to the heart of causality: the difference between an observation and an <em>intervention</em>.</p>
<p>When we introduced the ladder of causation, we mentioned the notion of <em>intervention</em>, something that changes the system. This is a fundamental operation, and it is important to understand the difference between intervention and observation. It may not at first seem natural to consider intervening as a fundamental action, evoking a similar sense of confusion as when one first encounters priors in Bayesian statistics. Is an intervention subjective? Who gets to define what an intervention is?</p>
<p>Simply, an intervention is a change to the data generating process. The joint distribution of the variables in the graph may be sampled from by simply “running the graph forward.” For each cause, we sample from its noise distribution and propagate that value through the SCM to calculate the resulting effects. To compute an <em>interventional</em> distribution, we force particular causes (on which we are intervening) to some value, and propagate those values through the equations of the SCM. This introduces a different distribution to the observational distribution we usually work with.</p>
<p>There is sometimes confusion between an interventional distribution and a conditional distribution. A conditional distribution is generated by filtering an observed distribution to meet some criteria. For instance, we might want to know the loan default rate among the businesses we have granted a loan to at a particular interest rate. This interest rate would itself likely have been determined by some model, and as such, the businesses with that rate will likely share statistical similarities.</p>
<p>The interventional distribution (when we intervene on interest rate) is fundamentally different. It is the distribution of loan defaulting if we <em>fix</em> the interest rate to a particular value, regardless of other features of the business that may warrant a different rate. This corresponds to removing all the inbound arrows to the interest rate in the causal graph - we’re forcing the value, so it no longer depends on its causal parents.</p>
<p>Clearly, not all interventions are physically possible! While we could intervene to set the interest rate, we would not be able to make every business a large one.</p>
<h4 id="interventions-in-code">Interventions in code</h4>
<div class="info">
<p>It is easy to make interventions concrete with code. Returning to the collider example, to compute an interventional distribution, we could define a new sampling function where instead of drawing all variables at random, we intervene to set <code>x</code> to a particular value. Because this is an intervention, not simply conditioning (as earlier), we must make the change, then run the data generating process again.</p>
<pre><code class="language-python">def sample_intervened():
  x_ = -3
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<p>Performing this intervention results in a new distribution for <code>z</code>, which is different from the observational distribution that we saw earlier.</p>
<p>[figure: interventional histogram]</p>
<p>Further, the relationship between x and y has changed; the joint distribution is now simply the marginal distribution of <code>y</code>, since <code>x</code> is fixed. This is a strikingly different relationship than when we simply conditioned the observational distribution.</p>
<p>[figure: interventional scatterplot]</p>
</div>
<h4 id="interventions-in-customer-churn">Interventions in customer churn</h4>
<p>In our <a href="https://ff06-2020.fastforwardlabs.com/">interpretability report</a>, we present a customer churn modeling use case. Briefly, given 20 features of customers of a telco - things like tenure, demographic attributes, whether they have phone and internet services and whether they have tech support - we must model their likelihood of churning within a fixed time. To do this, a dataset of customers and whether they churned in the time period is provided. This can be modeled as straightforward binary classification, and we can use the resulting output scores as a measure of how likely a customer is to churn.</p>
<p>The model used to calculate the churn score is an ensemble of a linear model, a random forest, and a simple feed forward neural network. With appropriate hyperparameters and training procedure, such an ensemble is capable of good predictive performance. That performance is gained by exploiting subtle correlations in the data.</p>
<p>To understand the predictions made, we apply <a href="https://arxiv.org/abs/1602.04938">LIME</a>. This returns a feature importance at the local level: which features contributed to each individual prediction. To accompany the analysis, we built Refractor, an interface for exploring the feature importances. Examining these is interesting, and highlights the factors that are <em>correlated</em> with a customer being likely to churn. <a href="https://refractor.fastforwardlabs.com/">Refractor</a> suggests which features most affect the churn prediction, and allows an analyst to change customer features and see the resulting churn prediction.</p>
<p>[figure: refractor]</p>
<p>Because we have a model that provides new predictions when we change the features, it is tempting to believe we can infer from this alone how to reduce churn probability. Aside from the fact that often the most important features cannot be changed by intervention (tenure, for instance), this is an incorrect interpretation of what LIME and our model provide. The correct interpretation of the prediction is the probability of churn for someone who <em>naturally</em> occurred in our dataset with those features, or, for instance, what this same customer’s churn probability will look like next year (when tenure will have naturally increased by one year), assuming none of their other features change.</p>
<p>Of course, there are some features that can be changed in reality. For instance:</p>
<ul>
<li>the telco could reduce the monthly fee for a customer,</li>
<li>or try to convince them to change contract type from monthly to yearly (one does not have to think too hard about why this changes the short-term churn probability), or</li>
<li>upgrade the service from DSL to fiber optic.</li>
</ul>
<p>Which of these interventions would most decrease the probability that the customer churns? We don’t know. Our model alone, for all its excellent predictive accuracy, can’t tell us that, precisely because it is entirely correlative. Even a perfect model, that 100% accurately predicts which customers will churn, cannot tell us that.</p>
<p>With some common sense, we can see that a causal interpretation is not appropriate here. LIME often reports that having a faster fiber optic broadband connection increases churn probability, relative to slower DSL. It seems unlikely that faster internet has this effect. In reality, LIME is correctly reporting that there is a <em>correlation</em> between having fiber optic and churning, likely because of some latent factors - perhaps people who prefer faster internet are also intrinsically more willing to switch providers. This distinction of interpretation is crucial.</p>
<p>The model can only tell us what <strong>statistical dependencies</strong> exist in the dataset we trained it on. The training dataset was purely observational - a snapshot of a window of time with observations about those customers in it. If we select “give the customer access to tech support” in the app, the model can tell us that similar customers who also had access to tech support were less likely to churn. Our model only captures information about customers who happened to have some combination of features. It does not capture information about what happens when we <em>change</em> a customer’s features. This is an important distinction.</p>
<p>To know what would happen when we intervene to change a feature, we must compute the interventional distribution (or a point prediction), which can be very different from the observational distribution. In the case of churn, it’s likely the true causal graph is rather complex.
Interpretability techniques such as LIME provide important insights into models, but they are not causal insights. To make good decisions using the output of any interpretability method, we need to combine it with causal knowledge.</p>
<p>Often, this causal knowledge is not formally specified in a graph, and we simply call it “domain knowledge,” or expertise. We have emphasized what the <em>model</em> cannot do, in order to make the technical point clear, but in reality, anyone working with the model would naturally apply their own expertise. The move from that to a causal model requires formally encoding the assumptions we make all the time and verifying that the expected statistical relationships hold in our observed data (and if possible, experimenting). Doing so would give us an understanding of the cause-effect relationships in our system, and the ability to reason quantitatively about the effect of interventions.</p>
<p>Constructing a useful causal model of churn is a complex undertaking, requiring both deep domain knowledge and a detailed technical understanding of causal inference.<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> In Chapter 3, we will discuss some techniques that are bridging the gap between a full causal model, and the supervised learning setup we use in problems like churn prediction.</p>
<h4 id="when-do-we-need-interventions%3F">When do we need interventions?</h4>
<p>When do we need to concern ourselves with intervention and causality? If all we want to do is predict, and to do so with high accuracy (or whatever model performance metric we care about), then we should use everything at our disposal to do so. That means making use of all the variables that may correlate with the outcome we’re trying to predict, and it doesn’t matter that they don’t cause the outcome. Correlation is not causation, but correlation is still predictive,<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> and supervised learning excels at discovering subtle correlations.</p>
<p>Some situations where this pure supervised learning approach is useful:</p>
<ul>
<li>We want to predict when a machine in our factory will fail.</li>
<li>We want to forecast next quarter’s sales.</li>
<li>We want to identify named entities in some text.</li>
</ul>
<p>Conversely, if we want to predict the effect of an intervention, we need causal reasoning. For example:</p>
<ul>
<li>We want to know what to change about our machines to reduce the likelihood of failures.</li>
<li>We want to know how we can increase next quarter’s sales.</li>
<li>We want to know whether longer or shorter article headlines generate more clicks.<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></li>
</ul>
<h3 id="how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</h3>
<p>Knowing the true causal structure of a problem is immensely powerful. Earlier in the chapter we discussed three building blocks of causal graphs - direct causation, forks and colliders - but for real problems, a graph can be arbitrarily complex.</p>
<p>The graph structure allows us to reason qualitatively about what statistical dependencies ought to hold in our data. In the absence of abundant randomized controlled trials or other experiments, qualitative thinking is necessary for causal inference. We must use our domain knowledge to construct a plausible graph to test against the data we have. It is possible to refute a causal graph by considering the statistical independence relations it implies, and matching those against the expected relations from the causal structure. For example, if two variables are connected by a common cause we have not conditioned on, we should expect a statistical dependence between them.</p>
<div class="info">
<h5 id="causal-discovery">Causal Discovery</h5>
<p>The independence relationships implied by a graph can be used for causal discovery. Causal discovery is the process of attempting to recover causal graphs from observational data. There are many approaches appropriate for different sets of assumptions about the graph. However, since many causal graphs can imply the same joint distribution, the best we should hope for from causal discovery is a set of plausible graphs, which, if we are fortunate, may contain the true graph. In reality, inferring the direction of causation in even a two variable system is not always possible from data alone. See <a href="http://jmlr.org/papers/v17/14-518.html">Distinguishing cause from effect using observational data: methods and benchmarks</a>.</p>
</div>
<p>It is not, in general, possible to <em>prove</em> a causal graph, since different graphs can result in the same observed and even interventional distributions. The difficulty of confirming a causal relationship means that we should always proceed with caution when making causal claims. It is best to think of causal models as giving results <em>conditional on a set of causal assumptions</em>. Two nodes that are not directly connected in the causal graph are assumed to be independent in the data generating process, except insofar as the causal relations described above (or combinations of them) induce a statistical dependence.</p>
<p>The validity of the results depends on the validity of the assumptions. Of course, we face the same situation in all machine learning work, and it is to be expected that stronger, causal claims require stronger assumptions than merely observational claims.</p>
<p>One case where we may be able to write down the true causal graph is when we have ourselves created the system. For instance, a manufacturing line may have a sufficiently deterministic process that it is possible to write down a precise graph encoding which parts move from which machine to another. If we were to model the production of faulty parts, that graph would be a good basis for the causal graph, since a machine that has not processed a given faulty part is unlikely to be responsible for the fault, and causal graphs encode exactly these independences.</p>
<h3 id="recap">Recap</h3>
<p>Causal graphical models present an intuitive and powerful means of reasoning about systems. If an application requires only pure prediction, this reasoning is not necessary, and we may apply supervised learning to exploit subtle correlations between variables and our predicted quantity of interest. However, when a prediction will be used to inform a decision that changes the system, or we want to predict for the system under intervention, we must reason causally  - or else likely draw incorrect conclusions. That said, behind every causal conclusion there is always a causal assumption that cannot be tested or verified by mere observation.</p>
<p>Even without a formal education in causal inference, there are advantages to the qualitative reasoning enabled by causal graphical models. Trying to write down a causal graph forces us to confront our mental model of a system, and helps to highlight potential statistical and interpretational errors. Further, it precisely encodes the independence assumptions we are making. However, these graphs could be complex and high dimensional and require close collaboration between practitioners and domain experts who have substantive knowledge of the problem.</p>
<p>In many domains, problems such as the large numbers of predictors, small sample sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of causal inference. In such cases, there is often limited background knowledge to reduce the space of alternative causal hypotheses. Even when experimental interventions are possible, performing the many thousands of experiments that would be required to discover causal relationships between thousands or tens of thousands of predictors is often not practical.</p>
<p>Given these challenges, how do we combine causal inference and machine learning? Many of the researched approaches at the intersection of ML and causal inference are motivated by the ability to apply causal inference techniques to high dimensional data, and in domains where specifying casual relationships could be difficult. In the next chapter, we will bridge this gap between structural causal models and supervised machine learning.</p>
<h2 id="causality-and-invariance">Causality and invariance</h2>
<p>Supervised machine learning is very good at prediction, but there are useful lessons we can take from causal models even for purely predictive problems.</p>
<p>Relative to recent advancements made in the broader field of machine learning, the intersection with causal reasoning is still in its infancy. Nonetheless, there are several emerging research directions. Here, we focus on one particularly promising path: the link between causality and invariance. Invariance is a desirable property for many machine learning systems: a model that is invariant is one that performs well in new circumstances, particularly when the underlying data distribution changes. As we will see in this Chapter, invariance also provides a route to some causal insights, even when working only with observational data.</p>
<h3 id="the-great-lie-of-machine-learning">The great lie of machine learning</h3>
<p>In supervised learning, we wish to predict something that we don’t know, based on only the information that we do have. Usually, this boils down to learning a mapping between input and output.</p>
<p>To create that map, we require a dataset of input features and output targets; the number of examples required scales with the complexity of the problem. We can then fit the parameters of a learning algorithm to the dataset to minimize some loss function that we choose. For instance, if we are predicting a continuous number, like temperature, we might seek to minimize the mean squared difference between the prediction and the true measurements.</p>
<p>If we are not careful, we will <em>over</em> fit the parameters of the ML algorithm to the dataset we train on. In this context, an overfit model is one that has learned the idiosyncrasies (the spurious correlations!) of our dataset. The result is that when the model is applied to any other dataset (even one with the same data generating process), the model’s performance is poor, because it is relying on superficial features that are no longer present.</p>
<p>To avoid overfitting, we employ various regularization schemes and adjust the capacity of the model to an appropriate level. When we fit the model, we shuffle and split our data, so we may learn the parameters from one portion of the data, and validate the resulting model’s performance on another portion. This gives us confidence that the learned parameters are capturing something about all the data we have, not merely a portion of it.</p>
<p>Whatever procedure we use (be it cross-validation, forward chaining for time series, or simpler train-test-validation splits), we are relying on a crucial assumption. The assumption is that the data points are <em>independent and identically distributed</em> ( i.i.d.). By <em>independent</em>, we mean that each data point was generated without reference to any of the others, and by <em>identically distributed</em>, we mean that the underlying distributions in the data generating process are the same for all the data points.</p>
<p>Paraphrasing <a href="https://www.youtube.com/watch?v=x1UByHT60mQ&amp;feature=youtu.be&amp;t=37m34s">Zoubin Ghahramani</a>,</p>
<blockquote>
<p>the i.i.d. assumption is the great lie of machine learning.</p>
</blockquote>
<p>Rarely are data truly independent and identically distributed. What are the ramifications of this misassumption for machine learning systems?</p>
<h3 id="dangers-of-spurious-correlations">Dangers of spurious correlations</h3>
<p>When we train a machine learning system with the i.i.d. assumption, we are implicitly assuming an underlying data generating process for that data. This data generating process defines an <em>environment</em>. Different data generating processes will result in different environments, with different underlying distributions of features and targets.</p>
<p>When the environment we predict in differs from the environment our machine learning system was trained on, we should expect it to perform poorly. The correlations between features and the target are different, and as such the model we created to map from features to target in one environment will output incorrect values of the target for the features in another environment.</p>
<p>Unfortunately, it’s rarely possible to know whether the data generating process for data at predict time - in a deployed ML system, for instance - will be the same as during training time. Even once the system is predicting in the wild, if we do not or cannot collect ground truth labels to match to the input features on which the prediction was based, we may never know.</p>
<p>This problem is not academic. <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> points this out in humorous fashion (see also <a href="http://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf">Unbiased Look at Dataset Bias</a>). The papers highlight that computer vision systems trained for visual recognition of objects, animals and people can utterly fail to recognise the same objects in different contexts. A cow on the slopes of an alpine field is easily recognised, but a cow on a beach is not noticed at all, or poorly classified as a generic “mammal.”</p>
<p>[figure: terra incognita cows]</p>
<p>These failures should not come as a surprise to us! Supervised machine learning is <em>designed</em> to exploit correlations between features to gain predictive performance, and cows and alpine pastures are highly correlated. Neural networks are a very flexible class of models that encode the invariants of the data set they’re trained on. If cows dominantly appear on grass, we should expect this to be learned.</p>
<div class="info">
<h5 id="when-is-a-correlation-spurious%3F">When is a correlation spurious?</h5>
<p>In supervised learning, we learn to use subtle correlations, possibly in high dimensional spaces like natural images, to make predictions. What distinguishes a genuine correlation from a spurious one? The answer depends on the intended use of the resulting model.</p>
<p>If we intend for our algorithm to work in only one environment, with very similar images, then we should use all the correlations at our disposal, including those that are very specific to our environment. However, if - as is almost always the case - we intend the algorithm to be used on new data outside of the training environment, we should consider any correlation that only holds in the training environment to be spurious. A spurious correlation is a correlation that only appears to be true due to a selection effect (such as selecting a training set!).</p>
<p>In the previous chapter, we saw that correlation can arise from several causal structures. In the strictest interpretation, any correlation that does not arise from direct causation could be considered spurious.</p>
<p>Unfortunately, given only a finite set of training data, it is often not possible to know which correlations are spurious. The methods in this section are intended to address precisely that problem.</p>
</div>
<p>When a machine learning algorithm relies heavily on spurious correlations for predictive performance, its performance will be poor on data from outside the set it was trained on. However, that is not the only problem with spurious correlations. There is an important and growing emphasis on interpretability in machine learning. A machine learning system should not only make predictions, but also provide means of inspecting how those predictions were made. If a model is relying on spurious correlations, the feature importances (such as those calculated by <a href="https://arxiv.org/abs/1602.04938">LIME</a> or <a href="https://arxiv.org/abs/1705.07874">SHAP</a>) will be similarly spurious. No one wants to make decisions based on spurious explanations!</p>
<h3 id="invariance">Invariance</h3>
<p>To be confident of our predictions outside of our training and testing datasets, we need a model that is robust to distributional shifts away from the training set. Such a model would have learned a representation which ignores dataset-specific correlations, and instead relies upon features that affect the target in all environments.</p>
<p>How can we go about creating such a model? We could simply train our model with data from multiple environments, as we often do in machine learning, playing fast and loose with the i.i.d. assumption. However, doing so naively would provide us with a model that can only generalize to the environments it has seen (and interpolations of them, if we use a robust objective; see <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a>). We wish our model to generalize beyond the limited set of environments we can access for training, and indeed extrapolate to new and unseen (perhaps unforeseen) environments. The property we are looking for - performing optimally in all environments - is called invariance.</p>
<p>The connection between causality and invariance is well established. In fact, causal relationships are by their nature invariant. The way many intuitive causal relationships are established is by observing that the relationship holds all the time, in all circumstances. Consider how physical laws are discovered. They are found by performing a series of experiments in different conditions, and monitoring which relationships hold, and what their functional form is. In the process of discovering nature’s laws, we will perform some tests that do not show the expected result. In cases where a law does not hold, this gives us information to refine the law to something that is invariant across environments.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></p>
<p>[figure: boiling water]</p>
<p>For example, water boils at 100 degrees Celsius. We could observe that everywhere, and write a simple causal graph: temperature → water boiling. We have learned a relationship that is invariant across all the environments we have observed.</p>
<p>Then, a new experiment conducted on top of a tall mountain reveals that on the mountain, water boils at a slightly lower temperature. After some more experimentation, we improve our causal model, by realising that in fact, both temperature and pressure affect the boiling point of water, and the true invariant relationship is more complicated.</p>
<p>The mathematics of causality make the notion of invariance and environments precise. Environments are defined by interventions in the causal graph. Each intervention changes the data generating process, such that the correlations between variables in the graph may be different (see Section <a href="#from-prediction-to-intervention">From prediction to intervention</a>). However, direct causal relationships are invariant relationships: if a node in the causal graph depends only on three variables, and our causal model is correct, it will depend on those three variables, and in the same way, regardless of any interventions. It may be that an intervention restricts the values that the causal variables take, but the relationship itself is not changed. Changing the arguments to a function does not change the function itself.</p>
<h4 id="invariance-and-machine-learning">Invariance and machine learning</h4>
<p>In the machine learning setting, we are mostly concerned with using features to predict a target. As such, we tend to select features for their predictive performance. In contrast, causal graphs are constructed based on domain knowledge and statistical independence relations, and thus encode a much richer dependency structure. However, we are not always interested in the entire causal graph. We may be interested only in the causes of a particular target variable. This puts us closer to familiar machine learning territory.</p>
<p>[figure: machine learning “graph” vs causal graph]
Fig. In supervised learning, we often use all available variables (or a subset selected for predictive performance) to predict an outcome. With structural causal models, we encode a much richer dependency structure between variables.</p>
<p>We will now examine two approaches to combining causal invariance and machine learning. The first, invariant causal prediction, uses the notion of invariance to infer the direct causes of a variable of interest. This restricted form of causal discovery - working out the structure of a small part of the graph we are interested in - is appropriate for problems with well defined variables where a structural causal model (or at least causal graph) could be created, in principle if not in practice.</p>
<p>Not all problems are amenable to SCMs. In the following section, we describe invariant risk minimization, where we forego the causal graph and seek to find a predictor that is invariant across multiple environments. We don’t learn anything about the graph structure from this procedure, but we do get a predictor with greatly improved out-of-distribution generalization.</p>
<h3 id="invariant-causal-prediction">Invariant Causal Prediction</h3>
<p><a href="https://arxiv.org/abs/1501.01332">Invariant causal prediction</a> (ICP) addresses the task of invariant prediction explicitly in the framework of structural causal models.</p>
<p>Often, the quantity we are ultimately concerned with in a causal analysis is the causal effect of an intervention: what is the difference in the target quantity when another variable is changed.<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> To calculate that, we either need to hold some other variables constant, or else account for the fact that they have changed. If we are only interested in the causes that affect a particular target, we do not need to construct the whole graph, but rather only determine which factors are the true direct causes of the target. Once we know that, we can answer causal questions, like how strongly each variable contributes to the effect, or the causal effect of changing one of the input variables.</p>
<p>The key insight offered by ICP is that because direct causal relationships are invariant, we can use that to determine the causal parents (the direct causes). The set up is similar to machine learning; we have some input features, and we’d like a model of an output target. The difference from supervised learning is that the goal is not performance at predicting the target variable. In ICP, we aim to discover the direct causes of a given variable - the variables that point directly into the target in the causal graph.</p>
<p>[figure: full causal graph to fewer assumption version]
Fig. We are not always interested in the full causal graph, and instead only seek to find the direct causes of a given target variable. This brings some of the advantages of a causal model into the supervised learning paradigm.</p>
<p>To use ICP, we take a target variable of interest, and construct a plausible list of the potential direct causes of that variable. Then we must define environments for the problem: each environment is a dataset. In the language of SCMs, each environment corresponds to data observed when a particular intervention somewhere in the graph was active. We can reason about this even without specifying the whole graph, or even which particular intervention was active, so long as we can separate the data into environments. In practice, we often take an observed variable to be the environment variable, when it could plausibly be so.</p>
<p>For instance, perhaps we are predicting sales volume in retail, and want to discern what store features causally impact sales. The target is sales volume, and the potential causes would be features like store size, number of nearby competitors, level of staffing, and so on.</p>
<p>Environments might be different counties (or even countries) - something that is unlikely to impact the sales directly, but which may impact the features that themselves impact the sales. For instance, different places will have different populations, and population density is a possible cause of sales volume. Importantly, the environment cannot be a descendent of the target variable.<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></p>
<p>[figure: two environments with same important features]
Fig. We fit a model in multiple environments, and monitor which features are consistently predictive.</p>
<p>To apply ICP, we first consider a subset of features. We then fit a linear (Gaussian) regression from this subset to the target in each environment we have defined. If the model does not change between environments (which can be assessed either via the coefficients or a check on residuals), we have found a set of features that appear to result in an invariant predictor. We iterate over subsets of features combinatorially. Features that appear in a model that is invariant are plausible causes of the target variable. The intersection of these sets of plausible causes (i.e. the features which are predictive in all environments) is then a subset of the true direct causes.</p>
<p>[figure: causal graph and ml with important features/causal parents highlighted]
Fig. The features that are consistently predictive of a target are likely the causal parents in the (unknown!) causal graph.</p>
<p>In machine learning terms, ICP is essentially a feature selection method, where the features selected are very likely to be the direct causes of the target. The model built atop those features can be interpreted causally: a high coefficient for a feature means that feature has a high causal effect on the target, and changes in those features should result in the predicted change in the target.</p>
<p>Naturally, there are some caveats and assumptions. In particular, we must assume that there is no unobserved confounding between the features and the target (recall, a confounder is a common cause of the feature and target). If there are known confounders, we must make some adjustments to account for them, detailed in the paper. The authors provide an R package, <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a>, implementing the methods.</p>
<p>The restriction of using a linear Gaussian model, and that environments be discrete, rather than defined by the value of a continuous variable, are removed by nonlinear ICP (see <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>). In the nonlinear case, we replace comparing residuals or coefficients with conditional independence tests.<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup></p>
<h3 id="invariant-risk-minimization">Invariant Risk Minimization</h3>
<p>When using Invariant Causal Prediction, we avoid writing the full structural causal model, or even the full graph of the system we are modeling, but we must still think about it.</p>
<p>For many problems, it’s difficult to even attempt drawing a causal graph. While structural causal models provide a complete framework for causal inference, it is often hard to encode known physical laws (such as Newton’s gravitation, or the ideal gas law) as causal graphs. In familiar machine learning territory, how does one model the causal relationships between individual pixels and a target prediction? This is one of the motivating questions behind the paper <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> (IRM). In place of structured graphs, the authors elevate invariance to the defining feature of causality.</p>
<p>They also make the connection between invariance and causality well:</p>
<blockquote>
<p>“If both Newton’s apple and the planets obey the same equations, chances are that gravitation is a thing.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Like ICP, IRM uses the idea of training in multiple environments. However, unlike ICP, IRM is not concerned with retrieving the causal parents of the target in a causal graph. Rather, IRM focusses on out-of-distribution generalization - the performance of a predictive model when faced with a new environment. The technique proposed aims to create a data representation, on which a classifier or regressor can perform optimally in all environments. The paper itself describes the IRM principle:</p>
<blockquote>
<p>“To learn invariances across environments, find a data representation such that the optimal classifier on top of that representation matches for all environments.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Said differently, the idea is that there is a latent causal structure behind the problem we’re learning, and the task is to recover a representation that encodes the part of that structure that affects the target. This is different from selecting features like in Invariant Causal Prediction. In particular, it provides a bridge from very low level features, such as individual pixels, to a representation encoding high level concepts, such as cows.</p>
<h4 id="the-causal-direction">The causal direction</h4>
<p>The idea of a latent causal system generating observed features is particularly useful as a view of computer vision problems. Computer vision researchers have long studied the generative processes involved in moving from real world objects to pixel representations.<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> It’s instructive to inspect the causal structure of a dataset of cow pictures.</p>
<p>[figure: causal direction with cows]</p>
<p>In nature, cows exist in fields and on beaches, and we have an intuitive understanding that the cow itself and the ground are different things. A neural network trying to predict the presence of a cow in an image could be called an “anti-causal” learning problem, because the direction of causation is the opposite of the direction of prediction. The presence of a cow causes certain pixel patterns, but pixels are the input to the network, and the presence of a cow is the output.</p>
<p>However, a further sophistication can be added: the dataset on which we train a neural network is not learning from nature, but rather from human provided annotations. This changes the causal direction - we are now learning the effect from the cause - since those annotations are caused by the pixels of the image. This is the view taken by IRM<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>, which thus interprets supervised learning from images as being a causal (rather than anticausal) problem.<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup></p>
<p>Not all supervised learning problems are causal. Anticausal supervised learning problems arise when the label is not provided based on the features, but by some other mechanism that causes the features. For example, in medical imaging, we could obtain a label without reference to the image itself by observing the case over time (this is not a recommended approach for treatment, of course).</p>
<p>Learning in the causal direction explains some of the success of supervised learning - there is a chance that it can recover invariant representations without modification. Any supervised learning algorithm is learning how to combine features to predict the target. If the learning direction is causal, each input is a potential cause of the output, and it’s possible that the features learned will be the true causes. The modifications that invariant risk minimization makes to the learning procedure improve the chance by specifically promoting invariance.</p>
<h3 id="how-irm-works">How IRM works</h3>
<p>To learn an invariant predictor, we must provide the IRM algorithm with data from multiple environments. As in ICP, these environments take the form of datasets, and as such the environments must be discrete. We need not specify the graphical or interventional structure associated with the environments. The motivating example of the IRM paper asks us to consider a machine learning system to distinguish cows from camels, highlighting a similar problem to that which <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> does - animals being classified based on their environment, rather than the animal. In this case, cows on sand may be misclassified as camels, due to the spurious correlations absorbed by computer vision systems.</p>
<p>[figure: two environments into one target]</p>
<p>Simply providing data from multiple environments is not enough. The problem of learning the optimal classifier in multiple environments is a bi-level constrained optimization problem, in which we must simultaneously find the optimal data representation and optimal classifier across multiple separate datasets. IRM reduces the problem to a single optimization loop, using a trick of introducing a constant classifier and introducing a new penalty term to the loss function.</p>
<pre><code>IRM loss = sum over environments (error + penalty)
</code></pre>
<p>The <code>error</code> is the usual error we would use for the problem at hand - for example, the cross entropy for a classification problem - calculated on each environment. The technical definition of the new <code>penalty</code> term is the squared gradient norm with respect to a constant classifier, but it has an intuitive explanation. While the error measures how well the model is performing in each environment, the penalty measures how much the performance could be improved in each environment with one gradient step.</p>
<p>By including the penalty term in the loss, we punish high gradients - situations where a large improvement in an environment would be possible with one more epoch of learning. The result is a model with optimal performance in all environments. Without the IRM penalty, a model could minimize the loss by performing extremely well in just one environment, and poorly in others. Adding a term to account for the model having a low gradient (roughly, it has converged) in each environment ensures that the learning is balanced between environments.</p>
<p>To understand the IRM paradigm, we can perform a thought experiment. Imagine we have a dataset of cows and camels, and we’d like to learn to classify them as such. We separate out the dataset by the geolocation of photos - those taken in grassy areas from one environment, and those taken in deserts form another.</p>
<p>As a baseline, we perform regular supervised learning to learn a binary classifier between cows and camels. The learning principle at work in supervised learning is referred to as <em>empirical risk minimization</em>, or ERM - we’re just seeking to minimize the usual cross entropy loss.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> We’ll surely find that we can get excellent predictive performance on these two environments, because we have explicitly provided data from both.</p>
<p>The trouble arises when we want to identify a cow on snow, and find that our classifier did not really learn to identify a cow. It learned to identify grass. The holdout performance of our model in any new environment we haven’t trained on will be poor.</p>
<p>[figure: three cows as input to model, predicting cow, polar bear and camel]</p>
<p>With IRM, we perform the training across (at least) two environments, and include the penalty term for each in the loss. We’ll almost certainly find that our performance in the training environments is reduced. However, because we have encouraged the learning of invariant features that transfer across environments, we’re more likely to be able to identify cows on snow. In fact, the very reason our performance in training is reduced is that we’ve not absorbed so many spurious correlations that would hurt prediction in new environments.</p>
<p>It is impossible to guarantee that a model trained with IRM learns <em>no</em> spurious correlations. That depends entirely on the environments provided. If a particular feature is a useful discriminator in all environments, it may well be learned as an invariant feature, even if in reality it is spurious. As such, access to sufficiently diverse environments is paramount for IRM to succeed.</p>
<p>However, we should not be reckless in labeling something as an environment. Both ICP and IRM note that splitting on arbitrary variables in observational data can create diverse environments while destroying the very invariances we wish to learn. While IRM promotes invariance as the primary feature of causality, it pays to hold a structural model in the back of one’s mind, and ask if an environment definition makes sense as something that would alter the data generating process.</p>
<h4 id="considerations-for-applying-irm">Considerations for applying IRM</h4>
<p>IRM buys us extrapolation powers to new datasets, where independent and identically distributed supervised learning can at best interpolate between them. Using it to construct models improves their generalization properties by explicitly promoting performance across multiple environments, and leaves us with a new, closer-to-causal representation of the input features. Of course, this representation may not be perfect - IRM is an optimization-based procedure, and we will never know if we have found the true minimum risk across all environments - but it should be a step towards latent causal structure. This means that we can use our model to predict based on true, causal correlations, rather than spurious, environment-specific correlations.</p>
<p>However, there is no panacea, and IRM comes with some challenges.</p>
<p>Often, the dataset that we use in a machine learning project is collected well ahead of time, and may have been collected for an entirely different purpose. Even when a well-labeled dataset that is amenable to the problem exists, it is seldom accompanied by detailed metadata (by which we mean information about the information). As such, we often do not have information about the environment in which the data was collected.</p>
<p>[figure: varied dataset with no metadata on environment]</p>
<p>Another challenge is finding data from sufficiently diverse environments. If the environments are similar, IRM will be unlikely to learn features that generalize to environments that are different. This is both a blessing and a curse - on the one hand, we do not need to have perfectly separated environments to benefit from IRM, but on the other, we are limited by the diversity of environments. If a feature appears to be a good predictor in all the environments we have, IRM will not be able to distinguish that from a true causal feature. In general, the more environments we have, and the more diverse they are, the better IRM will do at learning an invariant predictor, and the closer we get to a causal representation.</p>
<p>[figure: similar environments]</p>
<p>No model is perfect, and whether or not one is appropriate to use depends on the objective. IRM is more likely to produce an invariant predictor, with good out-of-distribution performance, than empirical risk minimization (regular supervised learning), but doing so will come at the expense of predictive performance in the training environment. It’s entirely possible that for a given application, we are very sure that the data in the eventual test distribution (“in the wild”) will be distributed in the same way as our training data. Further, we may know that all we want to do with the resulting model is predict, not intervene. If both these things are true, we should  stick to supervised learning with empirical risk minimization and exploit all the spurious correlations we can.</p>
<h2 id="prototype">Prototype</h2>
<p>Blah.</p>
<h2 id="landscape">Landscape</h2>
<p>Causality spans a broad area of topics, including using causal insights to improve machine learning methods, adapting it for high-dimensional datasets and applying them for better data-driven decision making in real-world contexts. We also discussed in Chapter 3 how the collected data is rarely an accurate reflection of the population, and hence may fail to generalize in different environments or new datasets. Methods based on invariance show promise in addressing out-of-distribution generalization.</p>
<h3 id="use-cases">Use Cases</h3>
<p>As we demonstrated in the <a href="#prototype">Prototype</a> Chapter, Invariant Risk Minimization is particularly well suited to image problems in diverse physical environments. However, an environment need not mean only the scenery in an image, and when it does, it need not be fixed to a single value. Here we suggest some applications in and beyond computer vision.</p>
<h4 id="healthcare">Healthcare</h4>
<p>In clinical imaging settings, radiologists manually annotate tissues, abnormalities, and pathologies of patients. Biomedical engineers then use these annotations to train systems to perform automatic tissue segmentation or pathology detection in medical images. Suppose a hospital installs a new MRI scanner. Unfortunately, due to the mechanical configuration, calibration, vendor and acquisition protocol of the scanner, the images it produces will differ from images produced by other scanners. Consequently, systems trained on data from other scanners would fail to perform well on the new scanner.</p>
<p>An algorithmic system based on invariant prediction that treats scanners as environments could find correspondences in images between scanners, and change its decisions accordingly. This could save the time, funds and energy needed to annotate images from the new scanner. <sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup></p>
<h4 id="robotics">Robotics</h4>
<p>[figure: robot]</p>
<p>Autonomous systems need to detect and adapt to different environments. For instance, autonomously following a man-made trail such as those normally traversed by hikers or mountain-bikers is a challenging and mostly unsolved task for robotics.<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup> Solving such a problem is important for many applications, including wilderness mapping and search and rescue. Moreover, following a trail would be the most efficient and safest way for a ground robot to travel medium and long distances in a forested environment: by their nature, trails avoid excessive slopes and impassable ground due to excessive vegetation or wetlands. Many robot types, including wheeled, tracked and legged vehicles, are capable of locomotion along realworld trails.</p>
<p>In order to successfully follow a forest trail, a robot has to perceive where the trail is, then react in order to stay on the trail. Perceiving real-world trails in natural conditions is an extremely difficult and interesting pattern recognition problem, which is often challenging even for humans. Unpaved roads are normally much less structured than paved ones: their appearance is very variable based on the wilderness area and often boundaries are not well defined. It would be impossible to capture comprehensive data about all trails. Casting the trail perception problem as an image classification task and adopting a method based on invariance that operates directly on the image’s raw pixel values would allow for out-of-distribution generalization to new trails. Naturally, similar ideas are relevant for autonomous vehicles in urban areas.</p>
<h4 id="activity-recognition-systems">Activity recognition systems</h4>
<p>The diversity of sensors on personal devices, such as smartphones and smartwatches, has driven the development of novel activity recognition technologies, which help capture a person’s daily lifestyle activities and gestures in the physical world. These systems utilize features defined over data from sensors, such as accelerometer or gyroscope, and are trained using data labeled explicitly with tags such ‘sitting’, ‘walking’ and ‘climbing’. Such capabilities enable many new applications from wellness tracking to immersive gaming.</p>
<p>Unfortunately, it is hard to satisfactorily model this data due to the diversity exhibited in the real world. At times the users can exhibit significant differences in the way they perform the same daily lifestyle activities, while at other times these devices may be unusually placed or a user may switch from one device to another which might degrade the system’s performance due to instance-specific variations. This also means that we may either need a labelled dataset that captures the activity for each user and device (which is prohibitively expensive) or another way of identifying attributes that generalize better. Methods based on invariance could be particularly useful and well suited for this.</p>
<h4 id="natural-language-processing">Natural language processing</h4>
<p>[figure: different types of text]</p>
<p>Invariant prediction approaches are of course not restricted exclusively to image problems. In natural language processing, texts from different publication platforms are tricky to analyze due to different contexts, vocabularies and differences between how authors express themselves. For instance, financial news articles use a vocabulary and tone that differs from that used biomedical research abstracts. Similarly, online movie reviews are linguistically different from tweets. Sentiment classification also relies heavily on context; different words are used to express whether someone likes a book versus an electronic gadget.
Two recent papers, <a href="https://arxiv.org/abs/2004.05007">An Empirical Study of Invariant Risk Minimization</a> and <a href="https://arxiv.org/abs/2003.09772">Invariant Rationalization</a>, apply the idea of IRM to sentiment classification task, and find it improves out of distribution generalization. In particular, invariance acts to remove spurious reliance on single words which correlate highly with the target. Like images, text corpora form very high dimensional datasets (there are many possible words!), making spurious correlations extremely unlikely to be noticed “manually”. As such, invariance based approaches are especially promising here.</p>
<h4 id="recommender-systems">Recommender systems</h4>
<p>[figure: recommender system]</p>
<p>Making good recommendations is an important problem on the web. In the recommendation problem, we observe how a set of users interact with a set of items, and our goal is to show each user a set of previously unseen items that s/he will like. Broadly speaking, recommendation systems use historical data to infer users’ preferences, and then use the inferred preferences to suggest items.</p>
<p>Traditionally, they use click (or ratings) data alone to infer user preferences. Click data expresses a binary decision about items - for example this can be clicking, purchasing, viewing—and we aim to predict unclicked items that she would want to click on. But this inference is biased by the exposure data: users do not consider each item independently at random. At times this assumption is mistaken, and overestimates the effect of the unclicked items. Some of these items—many of them, in large-scale settings—are unclicked because the user didn’t see them, rather than because she chose not to click them.</p>
<p>ReySys are a classic application for causality, which allows us to correct for this exposure bias by treating the selection of items to present to a user as an intervention. Applying causal approaches to recommendation naturally improves generalization to new data,<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> and it seems likely that methods seeking invariant prediction could enhance this.</p>
<h3 id="tools">Tools</h3>
<p>The invariance-based approaches to causality we have discussed do not require dedicated tooling - ICP and IRM are procedures that could be implemented with general purpose machine learning frameworks.</p>
<p>Nonetheless, the authors of the ICP papers <sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup> provide corresponding R packages: <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a> and <a href="https://cran.r-project.org/web/packages/nonlinearICP/index.html">nonlinearICP</a>. The packages make the techniques easy to use, and include additional utilities, such as dedicated plots for confidence intervals on causal coefficients. We are not aware of a package for IRM, but the authors have provided a <a href="http://github.com/facebookresearch/InvariantRiskMinimization/">code repository</a> which reproduces the paper results.</p>
<p>Below, we list a handful of open source projects that aid in traditional, SCM-based causal inference.</p>
<h4 id="dowhy">DoWhy</h4>
<p>Microsoft Research is developing the <a href="https://microsoft.github.io/dowhy/">DoWhy</a> python library for causal inference, incorporating elements of both causal graphical models and potential outcomes. The library is oriented around pandas DataFrames, and as such fits easily into a Python data analysis workflow. In particular DoWhy makes a separation between four stages of causal inference:</p>
<ol>
<li>Modeling - defining a causal graph, or else the assumptions necessary for a potential outcomes approach (the common causes of the treatment and the outcome variable).</li>
<li>Identification - identifying the expression it is necessary to evaluate, in terms of conditional probability distributions.</li>
<li>Estimation - estimating the treatment effect. There are many estimation methods available in DoWhy, including machine learning based methods from another of Microsoft’s causal libraries: <a href="https://github.com/microsoft/EconML">EconML</a></li>
<li>Refutation - assessing the robustness of the conclusion. Given the reliance of causal inference on modeling assumptions, it is especially important to find ways to test our conclusions. DoWhy provides several methods for this, such as introducing a dummy common cause or replacing the treatment with a random placebo.</li>
</ol>
<p>In addition to the above, DoWhy includes a novel algorithm, the “do-sampler.” In much of causal inference, the quantity of interest is a single number, for instance, the difference in the outcome variable when a binary treatment variable is applied (“what is the average causal effect of smoking on cancer incidence?”). The do-sampler extends the pandas DataFrame API directly, and moves beyond calculating causal effects to allow sampling from the full interventional distribution. Having done so, we can then compute arbitrary statistics under this intervention. The do-sampler is new, but provides a very promising direction for further research, and a potential avenue to making causal inference accessible to many more data science practitioners.</p>
<h4 id="causaldiscoverytoolbox">CausalDiscoveryToolbox</h4>
<p>The <a href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html">Causal Discovery Toolbox</a> provides implementations of many algorithms designed for causal discovery - attempting to recover the full causal graph from observational data alone. There are many approaches to causal discovery, and the library is relatively comprehensive, including both algorithms pairwise causal discovery (inferring the direction of causation between a pair of variables), graph skeleton creation (creating an undirected graph of potential causal relationships), and full graphical causal model discovery.</p>
<p>Discovery of entire causal graphs does not yet appear mature enough that we can naively trust it’s conclusions about the causal structure of a problem. This makes sense given the difficulty of the task! Inferring the whole causal structure from only observational data is about the hardest imaginable problem we could face with data!</p>
<h4 id="causalnex">CausalNex</h4>
<p><a href="https://causalnex.readthedocs.io/en/latest/">CausalNex</a> is a very recently released (at time of writing) toolkit to help data scientists do causal reasoning, by QuantumBlack. It provides both a graph structure learning component, to help build the causal graph, and tools to fit that graph as a Bayesian network.</p>
<p>The structure learning component is an implementation of <a href="https://arxiv.org/abs/1803.01422">DAGs with NOTEARS</a>, an algorithm that casts structure learning as a continuous optimization problem. In its simplest form, it assumes linear relationships between variables (but unlike some causal discovery methods, does not assume Gaussian noise). Further, the algorithm assumes that all variables are observed (ie. there is data for all variables). Unfortunately, this is rarely the case in causal problems.</p>
<p>Within these limitations, the algorithm is performant, and allows the user to specify hard constraints (such as “these variables cannot be child nodes”, or “there is no causal relationship between these two variables”). This facilitates directly encoding domain knowledge into the graph, and using the structure learning component as an aid in places where the causal connection is not known.</p>
<h4 id="pyro">Pyro</h4>
<p>Uber’s <a href="http://pyro.ai/">Pyro</a> probabilistic programming library is primarily intended for implementing deep probabilistic models and fitting them with variational inference. However, in addition to tools for conditioning on observed data, the library implements a do operation to force a variable to take a certain distribution. This allows simulating from interventional distributions, provided the structural causal model (including equations) is known. The intersection of probabilistic programming with causal inference is nascent, but promising!</p>
<h2 id="ethics">Ethics</h2>
<p>Machine learning is playing an increasingly critical role in our society. Decisions that were previously exclusively made by humans are more frequently being made algorithmically. These algorithmic systems govern everything from which emails reach our inboxes, to whether we are approved for credit, to whom we get the opportunity to date – and their impact on our experience of the world is growing. Furthermore, we still lack the understanding of how these machines work. We can neither explain nor correct them when they are acting unfairly or reinforcing biases. Causal reasoning gives us a framework for thinking about these problems. It provides a principled approach to assist the data scientist making an informed decision about the metric used to ascertain fairness while being fully aware of the tradeoffs involved with his/her choice.</p>
<h3 id="causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</h3>
<p>Even without employing the full machinery of causal inference, when one approaches a new problem, it can be informative to try to write down the causal graph. This forces us to confront our assumptions about a system. It also allows someone else to understand our assumptions, and gives a precise framework to debate.</p>
<p>[figure: people arguing over which direction an arrow goes]</p>
<p>Making our assumptions explicit aids transparency, which is a win. However, it doesn’t protect against bad assumptions. Establishing causal relationships is hard. Unless we are able to perform sufficient experiments to validate our hypotheses, causal reasoning from observational data is subject to untested (sometimes untestable) assumptions.</p>
<p>We should make any causal claim with humility. As ever, we should be careful of dressing up a bad analysis with additional formalism.</p>
<h3 id="omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</h3>
<p>Discrimination can be broadly grouped into two categories: direct and indirect. Direct discrimination is when indiviudals receive less favorable treatments on the basis of a protected attribute such as age, race, or disability. Some extreme cases include voting rights or unequal pay based on race or gender. Appropriately, these protected attributes are frequently omitted from machine learning systems. Using a protected attribute as a feature directly is inviting discrimination based on that attribute.</p>
<p>Indirect discrimination on the other hand is concerned with individuals who receive treatments on the basis of inadequately justified factors that are somewhat related to (but not the same as) the protected attribute. These cases are arguably more complex to characterize, and require more refined reasoning. One such scenario is wherein an insurance company denies coverage to residents in certain geographic locations. This isn’t a problem in itself, unless the majority of the residents belong to a certain ethnicity. In this case, the geographic location serves as a proxy for race. This is indirect <em>causal discrimination</em>.</p>
<p>Another sub-category of indirect discrimination is <em>spurious discrimination</em>. These are instances when there are no pathways from causal attributes to the outcome, but as we saw in <a href="#from-correlation-to-causation">From correlation to causation</a>, correlations can arise from numerous causal structures. As such, merely omitting the protected attribute does not omit its effects. A system is not guaranteed to be non-discriminatory on a protected attribute simply because it does not include that attribute directly. More simply, just because a feature does not cause the target does not mean that it will not be predictive of the target. This presents a particular challenge to algorithmic systems that are designed to find subtle correlations, especially since much historical data on which algorithms are trained is subject to selection bias (and other biases).</p>
<p>Since removing protected attributes is not enough, we must evaluate the resulting model for its discrimination and fairness properties. There are many possible measures of fairness, and it is generally impossible to optimize for all of them (see <a href="https://arxiv.org/abs/1609.05807">Inherent Trade-Offs in the Fair Determination of Risk Scores</a>).</p>
<p>Several recent papers (see <a href="https://arxiv.org/abs/1805.05859">Causal Reasoning for Algorithmic Fairness</a> and <a href="https://arxiv.org/abs/1706.02744">Avoiding Discrimination through Causal Reasoning</a>, for instance) have proposed causality as a route to understanding and defining fairness and discrimination. In particular, if we have a causal graphical model of a system, we can see which paths are impacted by protected attributes, and correctly account for that impact. There have also been contributions in non-parametric structural causal models that allow one to detect and distinguish the three main discriminations, namely, direct, indirect and spurious (see <a href="https://www.cs.purdue.edu/homes/eb/r30.pdf">Fairness in Decision-Making – The Causal Explanation Formula</a>).</p>
<p>That said, the difficulty lies in constructing the causal graph. A causal graph could of course be used to embed all kinds of biases and prejudices, but at least provides a basis for argument.</p>
<h3 id="invariance-as-a-route-to-fairness">Invariance as a route to fairness</h3>
<p>An interesting idea is proposed in the final section of the IRM paper: treating groups over which we want fairness as the environments. When we seek to learn an invariant model (be that by ICP or IRM), we are explicitly trying to learn a model that performs optimally in different environments. We could construct those environments by separating out groups having different values for protected attributes. Then, by learning a model that seeks to perform optimally in each environment, we are explicitly trying to guarantee the best performance for each protected attribute.</p>
<p>Said differently, invariant features are exactly those that are consistent across groups. Consider again a bank granting loans, this time directly to individuals. The bank does not wish to discriminate on the basis of protected attributes. By treating the protected attributes as the groups, they are looking to learn what impacts loan defaulting invariantly across those groups.</p>
<p>[figure: different types of cow]</p>
<p>The idea of learning an invariant predictor across environments is that the representation used is capturing something true about the generative process of the data. This representation would be, to some degree, <em>disentangled</em>, in the sense that each dimension of the representation (a vector) should correspond to something meaningful. <a href="https://arxiv.org/abs/1905.13662">On the Fairness of Disentangled Representations</a> shows experimentally that disentangled representations improve fairness in downstream uses.</p>
<h2 id="future">Future</h2>
<p>At the outset, causal reasoning provides a conceptual and technical framework for addressing questions about the effect of real or hypothetical actions or <em>interventions</em>. Once we understand what the effect of an action is, we can turn the question around and ask what action plausibly caused an event. This gives us a formal language to talk about cause-and-effect. That said, not every question about cause is easy to answer. Further, it may not be a trivial task to find an answer or even to interpret it. Causal graphs that we discuss in the <a href="#background%3A-causal-inference">Background: Causal Inference</a> chapter provide a convenient way to discuss these notions, and allow us to reason about statistical dependencies in observed data.</p>
<p>Structural causal models take a step further to this intuitive way of reasoning by making formal assumptions about the parametric form of how the variables interact.</p>
<p>However, causal graphs and SCMs become difficult to construct as the number of variables increases. Some systems are hard to model in this way. How do we draw a causal graph for pixels of an image? Or words in text? The problem gets out of hand quickly.</p>
<p>Fortunately, not all problems require the entire causal graph. Often, we are interested only in the causal relations associated with one particular target variable. This is where methods based on invariance (like IRM) step in to allow the model to capture stable features across environments (that is, different data generating processes). This paradigm enables out-of-distribution generalization. As opposed to causal graphs or structural causal models, where the only way to validate assumptions of the variable interactions is through experimentation, IRM allows us to test them on an unseen test set!</p>
<h3 id="comparable-approaches">Comparable approaches</h3>
<p>So, at this point we probably agree that methods based on invariance are promising. How else might we approach out-of-distribution generalization? In general, there are two families of approaches; those that learn to match the feature distributions (or estimate a data representation) and those that employ some kind of optimization technique.</p>
<h4 id="domain-adaptation">Domain adaptation</h4>
<p>Domain adaptation is a special case of transfer learning. In domain adaptation, the model learns a task in a source domain, which has some feature distribution, and we would like it to be able to perform the same task well in a target domain, where the feature distribution is different. Domains play the same role as environments in invariance-based approaches - a source domain is an environment that was trained in, and a target domain is any environment that was not trained in.</p>
<p>Domain adaptation also enforces a kind of invariance - it seeks a representation that is distributed the same across source and target domain (so, across environments).<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> However, truly invariant, causal features need not follow the same distribution in different environments. A snowy cow will not generate quite the same pixel distribution as a sandy cow, and the causal feature we wish to represent is the cow itself.</p>
<h4 id="robust-learning">Robust learning</h4>
<p>The idea of learning across multiple environments is not novel to invariance based approaches. <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a> is a family of techniques that uses the same multi-environment setup as IRM (but much predate it), with a similar goal of enabling or enhancing out-of-distribution generalization. Said differently, the goal is a predictor that is robust to distributional shifts of the inputs.</p>
<p>The difference from the IRM setup we have covered is the loss function. The key idea is to add environment-specific “baseline” terms to the loss, and try to fix these terms such that particularly noisy environments where the loss may be high do not dominate. Then minimizing the loss should guarantee good performance across all the known environments. Further, a robust predictor will perform well in new environments that are interpolations of those seen in training. This certainly improves out-of-distribution generalization, but does not allow <em>extrapolation</em> outside of what was seen in training, whereas IRM can extrapolate, thanks to relying on an invariant predictor.</p>
<h4 id="meta-learning">Meta-learning</h4>
<p>Approaches like domain adaptation, robust learning, and in general transfer learning try to alleviate the problem of out-of-distribution generalization to some extent. Unfortunately, learning invariant features with varying distributions across environments is still challenging. These approaches are good at interpolation, but not extrapolation.</p>
<p>This is where meta-learning approaches like Model Agnostic Meta Learning (MAML)<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup> come into play. The underlying idea for meta-learners generally is to attempt to learn tasks with a small number of labeled examples. Training meta-learners is a two-step process involving a <em>learner</em> and a <em>trainer</em>. The goal of the learner (model) is to quickly learn new tasks from a small amount of new data; hence, it is sometimes called a <em>fast learner</em>. (A task here refers to any supervised machine learning problem - e.g., predicting a class given a small number of examples.) This learner is trained, by the meta-learner, to be able to learn from a large number of different tasks. The meta-learner accomplishes this by repeatedly showing the learner hundreds and thousands of different tasks.</p>
<p>Learning then, happens at two levels. The first level focuses on quick acquisition of knowledge within each task with a few examples. The second level slowly pulls and digests information across all tasks. In case of MAML (which is optimization based), the learner (or the first level) can achieve an optimal fast learning on a new task with only a small number of gradient steps because the meta-learner provides a good initialization of a model’s parameters. This approach is close to the problem of learning an optimal classifier in multiple environments, and could be explored further to learn invariant features within the data.</p>
<p>Some recent works have made the connection between causality and meta-learning explicitly, see <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>.</p>
<h3 id="looking-ahead">Looking ahead</h3>
<p>In this section, we discuss future possibilities with causality in general as well as with methods based on invariance.</p>
<h4 id="causal-reinforcement-learning">Causal Reinforcement Learning</h4>
<p>Reinforcement learning is the study of how an agent can learn to choose actions that maximize its future rewards in an interactive and uncertain environment. These agents rely on plenty of simulations (and sometimes real data) to learn which actions lead to high reward in a particular context. Causality is also about calculating the effect of actions, and allows us to transfer knowledge to new, unfamiliar situations. These two disciplines have evolved independently with little interaction between them until recently. Integrating them is likely to be a fruitful area of research, and may extend the reach of both causality and reinforcement learning (RL).<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup></p>
<p>There is a natural mapping between the concept of intervention in causal inference and actions taken in RL. Throughout an episode of reinforcement learning (an episode is formed of one run of the system, for example, a complete game of chess, or go), an agent takes actions. This defines a data generating process for the reward that the agent ultimately cares about; different sequences of actions will generate different rewards. Since the agent can choose its actions, each of them is an intervention in this data generating process. In making this connection, we can leverage the mathematics of causal inference. For instance, we could use counterfactuals, the third level of the <a href="#the-ladder-of-causation">The ladder of causation</a>, to reason about actions not taken. Applying such causal techniques may reduce the state space the agent needs to consider, or help account for confounders.</p>
<p>Methods based on invariance, like IRM, in principle, learn to discover unknown invariances from multiple environments. We could leverage this attribute in reinforcement learning. An episode of RL consists of all the states that fall in between an initial state and a terminal state. Since each episode is independent of another, in IRM terminology they could be viewed as different environments.  An agent could then learn robust policies from each of these episodes that leverage the invariant part of behaviour or actions that lead to reward.</p>
<p>While reinforcement learning itself is still in nascent stages when it comes to commercial applications, combining it with causality offers a great potential.<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup> But prior to that, we need to address some questions. For example, how do we combine programming abstractions in causal modeling with reinforcement learning that can help find best decisions? What tools and libraries are necessary to enable commercial applications in this space?</p>
<h4 id="irm-and-environments">IRM and environments</h4>
<p>IRM uses the idea of training in multiple environments to achieve out-of-distribution generalization. Unfortunately, few datasets come with existing environment annotations. There are at least two ways we can try to address this problem.</p>
<p>The first is to be mindful of the environment when collecting data, and collect metadata alongside. This may be easy (for example, collecting the geo-location of photos in settings where this is possible and does not violate a user’s privacy), or extremely hard, and require much post-collection manual labelling.</p>
<p>Another compelling but untested option is to try combining IRM with some sort of clustering to segment a single dataset into environments.<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup> The question would be how to cluster in such a way that meaningful and diverse environments are defined. Since existing clustering approaches are purely correlative, and as such vulnerable to spurious correlations, this could prove challenging.</p>
<p>Studying the impact of environment selection, and how to create or curate datasets with multiple environments would be a valuable contribution to making invariance-based methods more widely applicable. The authors of <a href="https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization">An Empirical Study of Invariant Risk Minimization</a> reach the same conclusion.</p>
<h4 id="causal-reasoning-for-algorithmic-fairness">Causal reasoning for algorithmic fairness</h4>
<p>In the <a href="#ethics">Ethics</a> chapter we reviewed some notions of fairness in prediction problems and shared how tools of causal reasoning can be leveraged to address fairness. They depart in the traditional way of wholly relying on data-driven approaches and emphasize the need to require additional knowledge of the structure of the world, in the form of a causal model. This additional knowledge is particularly valuable as it informs us how changes in variables propagate in a system, be it natural, engineered or social. Explicit causal assumptions remove ambiguity from methods that just depend upon statistical correlations. Avoiding discrimination through causal reasoning is an active area of research. As efforts to aid more transparency and fairness in machine learning systems grow, causal reasoning will continue to gain significant momentum in guiding algorithms towards fairness.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>See for instance, recent works by Yoshua Bengio, like <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>See, for instance <a href="https://eng.uber.com/causal-inference-at-uber/">Using Causal Inference to Improve the Uber User Experience</a>. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Facebook performed <a href="https://www.pnas.org/content/111/24/8788">such an experiment</a> in 2012, and received <a href="https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/">much criticism</a> as a result. The ethical problem is not so much with the experiment itself, but rather that the subjects had not given informed consent, in violation of basic ethical guidelines for psychological research. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>An alternative popular framework is the Neyman-Reuben causal model, also known as <a href="https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB">Potential Outcomes</a>. The frameworks are equivalent in that they can compute the same things, though some causal queries may be easier to reason about in one or the other. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>See also Pearl’s article: <a href="https://cacm.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext">The Seven Tools of Causal Inference, with Reflections on Machine Learning</a>. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Some farm-experienced members of the CFF team are keen to point out that roosters crow pretty much <em>all the time</em>. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>On a technical note, correlation measures only <em>linear</em> association. For instance, <code>x</code> squared is uncorrelated with <code>x</code>, despite being completely dependent on it. When we say “correlation is not causation,” we really mean “statistical dependence is not causation”. <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p> Alas, it requires a far more detailed technical knowledge than we can provide in this report. We recommend the textbook <a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics: A Primer</a> for a succinct introduction to Structural Causal Models. An abbreviated overview (<a href="https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf">Causal Inference in Statistics: An Overview</a>) is freely available as a PDF. The textbook <a href="https://mitpress.mit.edu/books/elements-causal-inference">Elements of Causal Inference</a> (available through Open Access) also covers structural causal models, with additional links to machine learning. <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>We will examine the nuances of this statement in the next Chapter. Correlation is predictive in distribution. <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Adam Kelleher and Amit Sharma have an excellent <a href="https://medium.com/@akelleh/introducing-the-do-sampler-for-causal-inference-a3296ea9e78d">blog post</a> describing this problem, and introducing a new causal sampling technology to make solving it easier. <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>The scientific process of iterated hypothesis and experimentation can also be applied to constructing a causal model for business purposes. The popular George Edward Box quote is pertinent here: “all models are wrong, but some are useful”. <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Judea Pearl’s do-calculus is a set of rules to calculate exactly which variables we must account for, and how, to answer a given causal query in a potentially complicated graph. This is not trivial - often there are unobserved variables in a graph, and we must try to express the query only in terms of those variables for which we have data. <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>There is a subtlety here. We said environments were defined by interventions. Naturally, it is impossible to intervene on the country a store is built in once the store is built. This turns out not to matter for the purposes of inferring the direct causal parents of the sales volume, so long as the country is further up the graph, and changing country alters the data generating process. <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>Nonparametric conditional independence testing is an area of active research, and is generally hard, and made more so by having finite data. The nonlinear ICP paper also introduces the notion of defining sets - sometimes no single set of variables is accepted as the set of causal parents, but there are similar sets differing by only one or two variables that may be related. While the algorithm has failed to find a single consistent model, it is nonetheless conveying useful causal information. <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Longer than you may think! See, for instance, <a href="https://dspace.mit.edu/handle/1721.1/11589">Machine perception of three-dimensional solids</a>, published in 1963. <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>The final section of the IRM paper includes a charming socratic dialogue that discusses this distinction, as well as the reason that regular supervised learning is so successful, from an invariance standpoint. <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>See <a href="https://arxiv.org/abs/1206.6471">On Causal and Anticausal Learning</a> for a description of the insight considering the causal direction of a problem brings to machine learning. <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Technically, loss is the error on the training set, and risk is the error across the whole data distribution. With finite training data, minimizing the loss on the training set is a proxy for minimizing the risk. <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="https://ieeexplore.ieee.org/document/6945865">Transfer Learning Improves Supervised Image Segmentation Across Imaging Protocols</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="http://rpg.ifi.uzh.ch/docs/RAL16_Giusti.pdf">A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</a> <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>See <a href="http://www.its.caltech.edu/~fehardt/UAI2016WS/papers/Liang.pdf">Causal Inference for Recommendation</a> and <a href="https://arxiv.org/abs/1808.06581">The Deconfounded Recommender: A Causal Inference Approach to Recommendation</a>. <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://arxiv.org/abs/1501.01332">Causal inference using invariant prediction: identification and confidence intervals</a> and <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>. <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p><a href="https://arxiv.org/abs/1505.07818">Domain adversarial training of neural networks</a> <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p><a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>There is a nice introduction to causal reinforcement learning in the paper <a href="http://gershmanlab.webfactional.com/pubs/RL_causal.pdf">Reinforcement learning and causal models</a>. The blog post <a href="https://causallu.com/2018/12/31/introduction-to-causalrl/">Introduction to Causal RL</a> contains a shorter description, and also suggests some medical applications. <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>We are grateful to David Lopez-Paz (one of the <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> authors) for sharing his thoughts and ideas about possible extensions and applications of IRM with us, including applications to RL. <a href="#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>This idea was also suggested to us by David Lopez-Paz. <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        </div>
      </body>
      <!-- TODO: Google Analytics -->
      <script>
     </script>
      <!-- End Google Analytics -->
    </html>
  